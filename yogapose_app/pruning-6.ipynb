{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5206b3d",
   "metadata": {},
   "source": [
    "# Task 1: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbb3825",
   "metadata": {},
   "source": [
    "# 1. Set Up the Environment:\n",
    "\n",
    "- Install necessary libraries such as PyTorch and torchvision.\n",
    "- Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04897bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T10:20:42.014141Z",
     "iopub.status.busy": "2024-07-30T10:20:42.013504Z",
     "iopub.status.idle": "2024-07-30T10:24:26.634241Z",
     "shell.execute_reply": "2024-07-30T10:24:26.633038Z",
     "shell.execute_reply.started": "2024-07-30T10:20:42.014111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 160MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 200] loss: 1.226\n",
      "Accuracy of the network on the validation images after epoch 1: 70.34%\n",
      "[Epoch 2, Batch 200] loss: 0.783\n",
      "Accuracy of the network on the validation images after epoch 2: 75.76%\n",
      "[Epoch 3, Batch 200] loss: 0.668\n",
      "Accuracy of the network on the validation images after epoch 3: 77.43%\n",
      "[Epoch 4, Batch 200] loss: 0.594\n",
      "Accuracy of the network on the validation images after epoch 4: 77.54%\n",
      "[Epoch 5, Batch 200] loss: 0.562\n",
      "Accuracy of the network on the validation images after epoch 5: 79.57%\n",
      "[Epoch 6, Batch 200] loss: 0.515\n",
      "Accuracy of the network on the validation images after epoch 6: 79.96%\n",
      "[Epoch 7, Batch 200] loss: 0.482\n",
      "Accuracy of the network on the validation images after epoch 7: 79.10%\n",
      "[Epoch 8, Batch 200] loss: 0.467\n",
      "Accuracy of the network on the validation images after epoch 8: 79.50%\n",
      "[Epoch 9, Batch 200] loss: 0.442\n",
      "Accuracy of the network on the validation images after epoch 9: 81.54%\n",
      "[Epoch 10, Batch 200] loss: 0.428\n",
      "Accuracy of the network on the validation images after epoch 10: 80.32%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752577e3",
   "metadata": {},
   "source": [
    "# 2. Download and Prepare the CIFAR-10 Dataset:\n",
    "\n",
    "- Download the CIFAR-10 dataset using torchvision.datasets.\n",
    "- Split the dataset into training (40,000 images) and validation (10,000 images) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1822de03-7399-474c-a67a-d922cae11747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations for the training and validation datasets\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomCrop(32, padding=4),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Download and load the CIFAR-10 training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "\n",
    "# Download and load the CIFAR-10 test dataset\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "\n",
    "# Split the training dataset into training (40,000 images) and validation (10,000 images) sets\n",
    "train_size = 40000\n",
    "val_size = 10000\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Create data loaders for training, validation, and test datasets\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23369257-4a2f-4f4b-984f-ba4b37148039",
   "metadata": {},
   "source": [
    "# 3. Define the CNN Model:¶\n",
    "\n",
    "- Choose a CNN architecture (Resnet18).\n",
    "\n",
    "- Modify the last layer to have 10 output classes for the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc561134-5daa-497a-9dde-b3c8858089ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet18 model\n",
    "net = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the last layer to match the number of classes in CIFAR-10\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15ad2a-fa1a-41f2-ae3c-23e9efcbe997",
   "metadata": {},
   "source": [
    "# 4. Define Loss Function and Optimizer:\n",
    "\n",
    "- Use CrossEntropyLoss and an optimizer like SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8c2e1-c94e-476d-a0e7-9e7430a7d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9e021-5270-4280-b437-2ac1b6719daa",
   "metadata": {},
   "source": [
    "# 5. Train the Model:\n",
    "\n",
    "- Train the model for 10 epochs and evaluate on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526946bb-2879-4a09-9d4c-7e28cc8d79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af876b43-94c9-4754-ad8f-96d75e6ccbe5",
   "metadata": {},
   "source": [
    "# 6. Evaluate on the Test Set:\n",
    "\n",
    "- Report the accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec427761-3369-4f60-9171-40b532c50cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Evaluate on validation data after each epoch\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the validation images after epoch {epoch + 1}: {100 * correct / total:.2f}%')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5f7ab",
   "metadata": {},
   "source": [
    "# Task 2: Model Pruning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca339a5d-fe74-4242-b0c1-11921a643798",
   "metadata": {},
   "source": [
    "## 1. Apply Pruning Techniques & Evaluate Pruned Models:\n",
    "\n",
    "- Use PyTorch's pruning functionalities to prune the model.\n",
    "- Experiment with different pruning ratios.\n",
    "- Evaluate the pruned models on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26a40ad-a4e5-4c2f-bece-ac4d69971d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90c3bc2-24f1-411b-b583-b2fe2fedbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply pruning to the model\n",
    "def apply_pruning(model, amount):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f19b53-7cd5-46b5-a8de-2947723954e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove the pruning reparameterization\n",
    "def remove_pruning_reparameterization(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            try:\n",
    "                prune.remove(module, 'weight')\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e3bd6f-ca76-458e-950e-0d8c05b879c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of unmasked (non-zero) weights\n",
    "def count_unmasked_weights(model):\n",
    "    unmasked_weights = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            if hasattr(module, 'weight_mask'):\n",
    "                unmasked_weights += module.weight_mask.sum().item()\n",
    "            else:\n",
    "                unmasked_weights += module.weight.numel()\n",
    "    return unmasked_weights\n",
    "\n",
    "# Function to count the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec65fa3-11aa-45ff-861b-3b8b0ac47dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aafb826-bc93-46cc-b8f7-175f42e76265",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c7254-9459-4375-a0c7-42674f60ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and modify the final layer\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "model.to(device)\n",
    "\n",
    "# Load the trained model state dictionary\n",
    "model.load_state_dict(torch.load('original_model.pth'))\n",
    "\n",
    "# Evaluate the trained model\n",
    "original_accuracy = evaluate_model(model, valloader)\n",
    "print(f'Accuracy of the original model: {original_accuracy:.2f}%')\n",
    "\n",
    "# Save the trained model state dictionary\n",
    "torch.save(model.state_dict(), 'original_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df770c-7b12-49c8-811e-73d5e5a02875",
   "metadata": {},
   "source": [
    "## 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60b8c63a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T10:36:50.391510Z",
     "iopub.status.busy": "2024-07-30T10:36:50.391125Z",
     "iopub.status.idle": "2024-07-30T10:37:12.502929Z",
     "shell.execute_reply": "2024-07-30T10:37:12.501792Z",
     "shell.execute_reply.started": "2024-07-30T10:36:50.391481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the original model: 85.48%\n",
      "Original number of parameters: 11181642\n",
      "Original number of unmasked weights: 11172032\n",
      "Pruning ratio: 0.2\n",
      "Number of parameters: 11181642\n",
      "Number of unmasked weights: 8937625.0\n",
      "Pruning ratio: 0.3\n",
      "Number of parameters: 11181642\n",
      "Number of unmasked weights: 7820423.0\n",
      "Pruning ratio: 0.4\n",
      "Number of parameters: 11181642\n",
      "Number of unmasked weights: 6703219.0\n",
      "Pruning ratio: 0.6\n",
      "Number of parameters: 11181642\n",
      "Number of unmasked weights: 4468813.0\n",
      "Pruning ratio: 0.7\n",
      "Number of parameters: 11181642\n",
      "Number of unmasked weights: 3351609.0\n",
      "Pruning ratio: 0.8\n",
      "Number of parameters: 11181642\n",
      "Number of unmasked weights: 2234407.0\n",
      "Pruning ratio: 0.9\n",
      "Number of parameters: 11181642\n",
      "Number of unmasked weights: 1117203.0\n",
      "Accuracy of the pruned model with ratio 0.2: 85.07%\n",
      "Accuracy of the pruned model with ratio 0.3: 85.03%\n",
      "Accuracy of the pruned model with ratio 0.4: 84.21%\n",
      "Accuracy of the pruned model with ratio 0.6: 77.24%\n",
      "Accuracy of the pruned model with ratio 0.7: 53.36%\n",
      "Accuracy of the pruned model with ratio 0.8: 30.96%\n",
      "Accuracy of the pruned model with ratio 0.9: 17.12%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply pruning with multiple ratios\n",
    "pruning_ratios = [0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "pruned_models = {}\n",
    "original_num_params = count_parameters(model)\n",
    "original_num_unmasked_weights = count_unmasked_weights(model)\n",
    "print(f'Original number of parameters: {original_num_params}')\n",
    "print(f'Original number of unmasked weights: {original_num_unmasked_weights}')\n",
    "\n",
    "for ratio in pruning_ratios:\n",
    "    # Create a new model instance and load the trained state dictionary\n",
    "    model_copy = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    num_ftrs = model_copy.fc.in_features\n",
    "    model_copy.fc = nn.Linear(num_ftrs, 10)\n",
    "    model_copy.to(device)\n",
    "    \n",
    "    model_copy.load_state_dict(torch.load('original_model.pth'))  # Load the trained model\n",
    "\n",
    "    # Apply pruning\n",
    "    pruned_model = apply_pruning(model_copy, ratio)\n",
    "    \n",
    "    # Count the number of unmasked weights\n",
    "    num_unmasked_weights = count_unmasked_weights(pruned_model)\n",
    "    \n",
    "    # Remove reparameterization\n",
    "    pruned_model = remove_pruning_reparameterization(pruned_model)\n",
    "    \n",
    "    pruned_models[ratio] = pruned_model\n",
    "    \n",
    "    # Display number of parameters and number of unmasked weights\n",
    "    num_params = count_parameters(pruned_model)\n",
    "    print(f'Pruning ratio: {ratio}')\n",
    "    print(f'Number of parameters: {num_params}')\n",
    "    print(f'Number of unmasked weights: {num_unmasked_weights}')\n",
    "\n",
    "# Evaluate the pruned models\n",
    "for ratio, model in pruned_models.items():\n",
    "    accuracy = evaluate_model(model, valloader)\n",
    "    print(f'Accuracy of the pruned model with ratio {ratio}: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf51681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T10:38:18.868894Z",
     "iopub.status.busy": "2024-07-30T10:38:18.867817Z",
     "iopub.status.idle": "2024-07-30T10:39:39.788027Z",
     "shell.execute_reply": "2024-07-30T10:39:39.786840Z",
     "shell.execute_reply.started": "2024-07-30T10:38:18.868847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the original model: 85.16%\n",
      "Pruning ratio: 0.25, Accuracy: 85.11%, Unmasked weights: 8379024.0\n",
      "Pruning ratio: 0.26, Accuracy: 85.27%, Unmasked weights: 8267302.0\n",
      "Pruning ratio: 0.27, Accuracy: 85.11%, Unmasked weights: 8155587.0\n",
      "Pruning ratio: 0.28, Accuracy: 84.82%, Unmasked weights: 8043861.0\n",
      "Pruning ratio: 0.29, Accuracy: 85.14%, Unmasked weights: 7932141.0\n",
      "Pruning ratio: 0.3, Accuracy: 84.73%, Unmasked weights: 7820423.0\n",
      "Pruning ratio: 0.31, Accuracy: 85.26%, Unmasked weights: 7708703.0\n",
      "Pruning ratio: 0.32, Accuracy: 84.88%, Unmasked weights: 7596982.0\n",
      "Pruning ratio: 0.33, Accuracy: 85.07%, Unmasked weights: 7485262.0\n",
      "Pruning ratio: 0.33999999999999997, Accuracy: 84.70%, Unmasked weights: 7373540.0\n",
      "Pruning ratio: 0.35, Accuracy: 84.67%, Unmasked weights: 7261821.0\n",
      "Pruning ratio: 0.36, Accuracy: 85.01%, Unmasked weights: 7150100.0\n",
      "Pruning ratio: 0.37, Accuracy: 84.56%, Unmasked weights: 7038377.0\n",
      "Pruning ratio: 0.38, Accuracy: 84.76%, Unmasked weights: 6926663.0\n",
      "Pruning ratio: 0.39, Accuracy: 84.61%, Unmasked weights: 6814940.0\n",
      "Pruning ratio: 0.4, Accuracy: 84.54%, Unmasked weights: 6703219.0\n",
      "Pruning ratio: 0.41000000000000003, Accuracy: 84.50%, Unmasked weights: 6591500.0\n",
      "Pruning ratio: 0.42000000000000004, Accuracy: 84.60%, Unmasked weights: 6479778.0\n",
      "Pruning ratio: 0.43, Accuracy: 84.08%, Unmasked weights: 6368058.0\n",
      "Pruning ratio: 0.44, Accuracy: 84.47%, Unmasked weights: 6256337.0\n",
      "Pruning ratio: 0.45, Accuracy: 83.76%, Unmasked weights: 6144617.0\n",
      "Pruning ratio: 0.45999999999999996, Accuracy: 84.13%, Unmasked weights: 6032899.0\n",
      "Pruning ratio: 0.47, Accuracy: 83.44%, Unmasked weights: 5921179.0\n",
      "Pruning ratio: 0.48, Accuracy: 83.23%, Unmasked weights: 5809453.0\n",
      "Pruning ratio: 0.49, Accuracy: 82.90%, Unmasked weights: 5697738.0\n",
      "Pruning ratio: 0.5, Accuracy: 82.93%, Unmasked weights: 5586016.0\n",
      "Highest pruning ratio within 1% of the original accuracy: 0.44\n",
      "Accuracy at this pruning ratio: 84.47%\n",
      "Accuracy of the original model: 85.38%\n",
      "Original model p50 latency: 0.009817 seconds\n",
      "Original model p90 latency: 0.010340 seconds\n",
      "Accuracy of the pruned model with ratio 0.44: 84.40%\n",
      "Pruned model p50 latency: 0.009822 seconds\n",
      "Pruned model p90 latency: 0.010623 seconds\n",
      "\n",
      "Comparison:\n",
      "Original model p50 latency: 0.009817 seconds, p90 latency: 0.010340 seconds\n",
      "Pruned model p50 latency: 0.009822 seconds, p90 latency: 0.010623 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Initialize the model and modify the final layer\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "model.to(device)\n",
    "\n",
    "# Load the trained model state dictionary\n",
    "model.load_state_dict(torch.load('original_model.pth'))\n",
    "\n",
    "# Evaluate the trained model\n",
    "original_accuracy = evaluate_model(model, valloader)\n",
    "print(f'Accuracy of the original model: {original_accuracy:.2f}%')\n",
    "\n",
    "# Set the target accuracy\n",
    "target_accuracy = original_accuracy - 1.0\n",
    "\n",
    "# Apply pruning with refined ratios between 0.25 and 0.5\n",
    "pruning_ratios = [0.25 + 0.01 * i for i in range(26)]  # Pruning ratios from 0.25 to 0.5\n",
    "best_ratio = 0\n",
    "best_accuracy = 0\n",
    "for ratio in pruning_ratios:\n",
    "    # Create a new model instance and load the trained state dictionary\n",
    "    model_copy = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    num_ftrs = model_copy.fc.in_features\n",
    "    model_copy.fc = nn.Linear(num_ftrs, 10)\n",
    "    model_copy.to(device)\n",
    "    \n",
    "    model_copy.load_state_dict(torch.load('original_model.pth'))  # Load the trained model\n",
    "\n",
    "    # Apply pruning\n",
    "    pruned_model = apply_pruning(model_copy, ratio)\n",
    "    \n",
    "    # Count the number of unmasked weights\n",
    "    num_unmasked_weights = count_unmasked_weights(pruned_model)\n",
    "    \n",
    "    # Remove reparameterization\n",
    "    pruned_model = remove_pruning_reparameterization(pruned_model)\n",
    "    \n",
    "    # Evaluate the pruned model\n",
    "    accuracy = evaluate_model(pruned_model, valloader)\n",
    "    print(f'Pruning ratio: {ratio}, Accuracy: {accuracy:.2f}%, Unmasked weights: {num_unmasked_weights}')\n",
    "    \n",
    "    if accuracy >= target_accuracy:\n",
    "        best_ratio = ratio\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "print(f'Highest pruning ratio within 1% of the original accuracy: {best_ratio}')\n",
    "print(f'Accuracy at this pruning ratio: {best_accuracy:.2f}%')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate the model and record latencies\n",
    "def evaluate_model_with_latency(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            start_time = time.time()\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            end_time = time.time()\n",
    "            latencies.append(end_time - start_time)\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, latencies\n",
    "\n",
    "# Function to calculate p50 and p90 latencies\n",
    "def calculate_p50_p90(latencies):\n",
    "    latencies = np.array(latencies)\n",
    "    p50 = np.percentile(latencies, 50)\n",
    "    p90 = np.percentile(latencies, 90)\n",
    "    return p50, p90\n",
    "\n",
    "# Initialize the model and modify the final layer\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "model.to(device)\n",
    "\n",
    "# Load the trained model state dictionary\n",
    "model.load_state_dict(torch.load('original_model.pth'))\n",
    "\n",
    "# Evaluate the trained model and record latencies\n",
    "original_accuracy, original_latencies = evaluate_model_with_latency(model, valloader)\n",
    "print(f'Accuracy of the original model: {original_accuracy:.2f}%')\n",
    "original_p50, original_p90 = calculate_p50_p90(original_latencies)\n",
    "print(f'Original model p50 latency: {original_p50:.6f} seconds')\n",
    "print(f'Original model p90 latency: {original_p90:.6f} seconds')\n",
    "\n",
    "# Apply pruning with the optimal ratio (0.45)\n",
    "pruning_ratio = best_ratio\n",
    "\n",
    "# Create a new model instance and load the trained state dictionary\n",
    "pruned_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = pruned_model.fc.in_features\n",
    "pruned_model.fc = nn.Linear(num_ftrs, 10)\n",
    "pruned_model.to(device)\n",
    "\n",
    "pruned_model.load_state_dict(torch.load('original_model.pth'))  # Load the trained model\n",
    "\n",
    "# Apply pruning\n",
    "pruned_model = apply_pruning(pruned_model, pruning_ratio)\n",
    "\n",
    "# Remove reparameterization\n",
    "pruned_model = remove_pruning_reparameterization(pruned_model)\n",
    "\n",
    "# Evaluate the pruned model and record latencies\n",
    "pruned_accuracy, pruned_latencies = evaluate_model_with_latency(pruned_model, valloader)\n",
    "print(f'Accuracy of the pruned model with ratio {pruning_ratio}: {pruned_accuracy:.2f}%')\n",
    "pruned_p50, pruned_p90 = calculate_p50_p90(pruned_latencies)\n",
    "print(f'Pruned model p50 latency: {pruned_p50:.6f} seconds')\n",
    "print(f'Pruned model p90 latency: {pruned_p90:.6f} seconds')\n",
    "\n",
    "# Compare the latencies\n",
    "print(\"\\nComparison:\")\n",
    "print(f'Original model p50 latency: {original_p50:.6f} seconds, p90 latency: {original_p90:.6f} seconds')\n",
    "print(f'Pruned model p50 latency: {pruned_p50:.6f} seconds, p90 latency: {pruned_p90:.6f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a5246",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "The objective of this experiment was to evaluate the effectiveness of pruning techniques on a ResNet-18 model trained on the CIFAR-10 dataset. The goal was to reduce the model size while maintaining its accuracy within 1% of the original model. \n",
    "\n",
    "Here are the key findings and conclusions from the experiment:\n",
    "\n",
    "1. **Original Model Performance**:\n",
    "    - **Accuracy**: The original ResNet-18 model achieved an accuracy of 85.38% on the validation set.\n",
    "    - **Number of Parameters**: The original model had 11,181,642 parameters.\n",
    "    - **Number of Unmasked Weights**: The original model had 11,181,642 unmasked weights, as it was not pruned.\n",
    "    - **Latencies**:\n",
    "        - **p50 Latency**: 0.009817 seconds\n",
    "        - **p90 Latency**: 0.010340 seconds\n",
    "\n",
    "2. **Pruning Experiment**:\n",
    "    - Pruning was performed with various ratios ranging from 0.2 to 0.5.\n",
    "    - The best pruning ratio that maintained the accuracy within 1% of the original model was found to be **0.44**.\n",
    "    - At this pruning ratio, the pruned model achieved an accuracy of **84.47%**, which is within the 1% target accuracy threshold of the original model.\n",
    "\n",
    "3. **Comparison of Unmasked Weights**:\n",
    "    - **Original Model**: 11,181,642 unmasked weights.\n",
    "    - **Pruned Model (0.44 Ratio)**: 6,256,337 unmasked weights.\n",
    "\n",
    "4. **Latency Comparison**:\n",
    "    - **Original Model**:\n",
    "        - **p50 Latency**: 0.009817 seconds\n",
    "        - **p90 Latency**: 0.010340 seconds\n",
    "    - **Pruned Model (0.44 Ratio)**:\n",
    "        - **p50 Latency**: 0.009822 seconds\n",
    "        - **p90 Latency**: 0.010623 seconds\n",
    "\n",
    "5. **Conclusions**:\n",
    "    - The pruning technique successfully reduced the number of unmasked weights by approximately 44%, significantly decreasing the model's complexity.\n",
    "    - The pruned model's accuracy was maintained within 1% of the original model's accuracy, demonstrating the effectiveness of the pruning process.\n",
    "    - The latency analysis indicated that the pruned model had slightly higher p50 and p90 latencies compared to the original model. However, the increase in latency was minimal, showing that pruning did not substantially affect the model's inference speed.\n",
    "\n",
    "Overall, the experiment demonstrates that pruning can effectively reduce the model size while maintaining high accuracy and acceptable latency. This technique is beneficial for deploying models in resource-constrained environments where model size and inference speed are critical factors."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
