{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8auesZ7DHQSk"
   },
   "source": [
    "# Task 1: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdvqY40uH-yE"
   },
   "source": [
    "## 1. Set Up the Environment:\n",
    "\n",
    "- Install necessary libraries such as PyTorch and torchvision.\n",
    "- Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-YdZRn6IRO6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f059S7ZCIYfz"
   },
   "source": [
    "## 2. Download and Prepare the CIFAR-10 Dataset:\n",
    "\n",
    "- Download the CIFAR-10 dataset using torchvision.datasets.\n",
    "- Split the dataset into training (40,000 images) and validation (10,000 images) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4a-3fAoI_9r",
    "outputId": "9c6744f7-4502-4ed2-c4d9-004c3f323e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:18<00:00, 9212764.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download the CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_size = int(0.8 * len(trainset))   # training split : 40,000 images\n",
    "val_size = len(trainset) - train_size  # validation split : 10,000 images\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs_EU_yJJCpK"
   },
   "source": [
    "## 3. Define the CNN Model:\n",
    "- Choose a CNN architecture (VGG-16).\n",
    "- Modify the last layer to have 10 output classes for the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHnHaKX1JhpC",
    "outputId": "b1eaf37e-1a10-4186-a0b0-183f4ea0468b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:03<00:00, 173MB/s]\n"
     ]
    }
   ],
   "source": [
    "net = models.vgg16(pretrained=True)\n",
    "net.classifier[6] = nn.Linear(net.classifier[6].in_features, 10)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeT6TQovJjWg"
   },
   "source": [
    "## 4. Define Loss Function and Optimizer:\n",
    "\n",
    "- Use CrossEntropyLoss and an optimizer like SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YG9vWR0J3XQ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYO0XLTuJ0SR"
   },
   "source": [
    "## 5. Train the Model:\n",
    "\n",
    "- Train the model for 10 epochs and evaluate on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFtDKUPWKboa",
    "outputId": "a05ba9f5-203e-400c-b757-c76c958cd4c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Batch: 100] loss: 1.300\n",
      "[Epoch: 1, Batch: 200] loss: 0.782\n",
      "[Epoch: 1, Batch: 300] loss: 0.702\n",
      "[Epoch: 1, Batch: 400] loss: 0.629\n",
      "Accuracy on validation images after epoch 1: 79.13%\n",
      "[Epoch: 2, Batch: 100] loss: 0.514\n",
      "[Epoch: 2, Batch: 200] loss: 0.501\n",
      "[Epoch: 2, Batch: 300] loss: 0.482\n",
      "[Epoch: 2, Batch: 400] loss: 0.453\n",
      "Accuracy on validation images after epoch 2: 82.91%\n",
      "[Epoch: 3, Batch: 100] loss: 0.370\n",
      "[Epoch: 3, Batch: 200] loss: 0.360\n",
      "[Epoch: 3, Batch: 300] loss: 0.367\n",
      "[Epoch: 3, Batch: 400] loss: 0.369\n",
      "Accuracy on validation images after epoch 3: 83.65%\n",
      "[Epoch: 4, Batch: 100] loss: 0.293\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'[Epoch: {epoch + 1}, Batch: {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Evaluate the model on the validation set after each epoch\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on validation images after epoch {epoch + 1}: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ebiPMZZKNHy"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqT2f_KyG-b4",
    "outputId": "40dc6a68-c2a0-4df4-9db2-d96f32236169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 85.85%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on the validation dataset and report the accuracy achieved\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij-DS6XfKs3h"
   },
   "source": [
    "# Task 2: Model Pruning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1kbRjLIK6pw"
   },
   "source": [
    "## 1. Apply Pruning Techniques & Evaluate Pruned Models:\n",
    "\n",
    "- Use PyTorch's pruning functionalities to prune the model.\n",
    "- Experiment with different pruning ratios.\n",
    "- Evaluate the pruned models on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJEA4yGjG-b5"
   },
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xl1JQ7FWCh1t"
   },
   "outputs": [],
   "source": [
    "# Function to apply pruning to each layer in the model\n",
    "def prune_model(model, amount):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Jxen_eBCkCI"
   },
   "outputs": [],
   "source": [
    "# Function to remove pruning re-parametrization\n",
    "def remove_pruning(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIJ0nVUcCmD8"
   },
   "outputs": [],
   "source": [
    "# Evaluate model function\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    model.train()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duQV6LnRCpOG"
   },
   "outputs": [],
   "source": [
    "  # Experiment with 20 different pruning ratios and evaluate the model\n",
    "ratios = np.linspace(0.30, 0.95, 20)  # 20 values from 30% to 95% pruning\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RHU911QCrEN",
    "outputId": "e5830397-39a6-499c-94a8-7937ef29a8c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning ratio 0.30: Accuracy on validation images: 86.00%\n",
      "Pruning ratio 0.30: Accuracy on test images: 86.04%\n",
      "Pruning ratio 0.33: Accuracy on validation images: 86.03%\n",
      "Pruning ratio 0.33: Accuracy on test images: 85.80%\n",
      "Pruning ratio 0.37: Accuracy on validation images: 85.52%\n",
      "Pruning ratio 0.37: Accuracy on test images: 85.58%\n",
      "Pruning ratio 0.40: Accuracy on validation images: 85.42%\n",
      "Pruning ratio 0.40: Accuracy on test images: 85.20%\n",
      "Pruning ratio 0.44: Accuracy on validation images: 84.76%\n",
      "Pruning ratio 0.44: Accuracy on test images: 84.58%\n",
      "Pruning ratio 0.47: Accuracy on validation images: 84.26%\n",
      "Pruning ratio 0.47: Accuracy on test images: 84.04%\n",
      "Pruning ratio 0.51: Accuracy on validation images: 82.49%\n",
      "Pruning ratio 0.51: Accuracy on test images: 82.72%\n",
      "Pruning ratio 0.54: Accuracy on validation images: 80.38%\n",
      "Pruning ratio 0.54: Accuracy on test images: 80.15%\n",
      "Pruning ratio 0.57: Accuracy on validation images: 78.48%\n",
      "Pruning ratio 0.57: Accuracy on test images: 78.54%\n",
      "Pruning ratio 0.61: Accuracy on validation images: 75.15%\n",
      "Pruning ratio 0.61: Accuracy on test images: 75.06%\n",
      "Pruning ratio 0.64: Accuracy on validation images: 71.02%\n",
      "Pruning ratio 0.64: Accuracy on test images: 71.11%\n",
      "Pruning ratio 0.68: Accuracy on validation images: 61.81%\n",
      "Pruning ratio 0.68: Accuracy on test images: 62.16%\n",
      "Pruning ratio 0.71: Accuracy on validation images: 49.03%\n",
      "Pruning ratio 0.71: Accuracy on test images: 49.19%\n",
      "Pruning ratio 0.74: Accuracy on validation images: 33.76%\n",
      "Pruning ratio 0.74: Accuracy on test images: 33.54%\n",
      "Pruning ratio 0.78: Accuracy on validation images: 20.77%\n",
      "Pruning ratio 0.78: Accuracy on test images: 20.93%\n",
      "Pruning ratio 0.81: Accuracy on validation images: 14.06%\n",
      "Pruning ratio 0.81: Accuracy on test images: 14.36%\n",
      "Pruning ratio 0.85: Accuracy on validation images: 11.75%\n",
      "Pruning ratio 0.85: Accuracy on test images: 11.01%\n",
      "Pruning ratio 0.88: Accuracy on validation images: 10.62%\n",
      "Pruning ratio 0.88: Accuracy on test images: 10.02%\n",
      "Pruning ratio 0.92: Accuracy on validation images: 10.62%\n",
      "Pruning ratio 0.92: Accuracy on test images: 10.00%\n",
      "Pruning ratio 0.95: Accuracy on validation images: 10.62%\n",
      "Pruning ratio 0.95: Accuracy on test images: 10.00%\n"
     ]
    }
   ],
   "source": [
    "for ratio in ratios:\n",
    "    model_copy = models.vgg16(pretrained=True)\n",
    "    model_copy.classifier[6] = nn.Linear(model_copy.classifier[6].in_features, 10)\n",
    "    model_copy.load_state_dict(net.state_dict())\n",
    "    model_copy.to(device)\n",
    "\n",
    "    # Apply pruning\n",
    "    prune_model(model_copy, ratio)\n",
    "\n",
    "    # Evaluate pruned model on validation set\n",
    "    val_accuracy = evaluate_model(model_copy, valloader)\n",
    "    print(f'Pruning ratio {ratio:.2f}: Accuracy on validation images: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Evaluate pruned model on test set\n",
    "    test_accuracy = evaluate_model(model_copy, testloader)\n",
    "    print(f'Pruning ratio {ratio:.2f}: Accuracy on test images: {test_accuracy:.2f}%')\n",
    "\n",
    "    results[ratio] = (val_accuracy, test_accuracy)\n",
    "\n",
    "    # Save the pruned model with a unique name\n",
    "    pruned_model_filename = f'pruned_vgg16_cifar10_{ratio:.2f}.pth'\n",
    "    torch.save(model_copy.state_dict(), pruned_model_filename)\n",
    "\n",
    "    # Remove pruning re-parametrization\n",
    "    remove_pruning(model_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoWC-A1_7l7j"
   },
   "source": [
    "**As pruning ratio is increasing the Accuracy is decreasing however it is maximum at 44% **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEGJMX4vMTvo"
   },
   "source": [
    "## 2.  Choose Best pruning ratio & Save the Model\n",
    "- choose the best pruning ratio.\n",
    "- Save the original and pruned models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCfXSrY8MsLm",
    "outputId": "a86852fe-3bc8-4cc2-f2f1-46508ddce061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pruning ratio: 0.33 with accuracy: 86.03%\n",
      "Saved the best pruned model as best_pruned_vgg16_cifar10_0.33.pth\n",
      "Saved the original model as model.pth\n"
     ]
    }
   ],
   "source": [
    "# Choose the best pruning ratio\n",
    "best_ratio = max(results, key=results.get)\n",
    "print(f'Best pruning ratio: {best_ratio:.2f} with accuracy: {results[best_ratio][0]:.2f}%')\n",
    "\n",
    "# Save the best pruned model\n",
    "best_pruned_model_filename = f'best_pruned_vgg16_cifar10_{best_ratio:.2f}.pth'\n",
    "torch.save(model_copy.state_dict(), best_pruned_model_filename)\n",
    "print(f'Saved the best pruned model as {best_pruned_model_filename}')\n",
    "\n",
    "# Save the Original model\n",
    "torch.save(net.state_dict(), 'model.pth')\n",
    "print(f'Saved the original model as model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4P7aKeXM8VO"
   },
   "source": [
    "## 3. Evaluate P50 and P90 Performance:\n",
    "- evaluate the P50 and P90 of the model performance before and after pruning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKsbsAkZNV6n",
    "outputId": "c1e23c02-e467-45c1-ee90-c6a5efb60561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p50 accuracy: 73.09%, p90 accuracy: 85.60%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the pruned model on p50, p90 performance\n",
    "p50, p90 = np.percentile(list(results.values()), [50, 90])\n",
    "print(f'p50 accuracy: {p50:.2f}%, p90 accuracy: {p90:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "AuERIANNpPwG",
    "outputId": "c73f3d70-5dc9-4d2f-f446-4388411157c8"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_964fdf1e-9a63-46d8-adb9-85d268479c50\", \"model.pth\", 537215534)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_b6dd5206-e731-4156-ad07-6c8d460e7aa2\", \"pruned_vgg16_cifar10_0.40.pth\", 1074380206)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('model.pth')\n",
    "files.download('pruned_vgg16_cifar10_0.40.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QR6UfkNpv3n"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming 'net' is your trained model\n",
    "model_path = '/content/model.pth'\n",
    "pruned_model_path = '/content/pruned_vgg16_cifar10_0.40.pth'\n",
    "\n",
    "torch.save(net.state_dict(), model_path)\n",
    "torch.save(pruned_net.state_dict(), pruned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "p1aq6v_RqB1I",
    "outputId": "3e1041ef-94ad-4688-e15a-34de07c981c9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-55ca6286e3e4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hi' is not defined"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIHShk3hNk1O"
   },
   "source": [
    "### Summary Report\n",
    "\n",
    "#### Original Model\n",
    "- Accuracy on Validation Dataset: 86.1%\n",
    "- Accuracy on Test Dataset: 85.85%\n",
    "\n",
    "#### Pruned Model\n",
    "- Pruning Ratio: 0.4\n",
    "- Accuracy on Validation Dataset: 85.42%\n",
    "- Accuracy on Test Dataset: 85.2%\n",
    "- Reduction in Model Size: 40%\n",
    "\n",
    "#### P50 and P90 Performance\n",
    "\n",
    "- Pruned Model: P50: 73.09%, P90: 85.6%\n",
    "\n",
    "### Conclusion\n",
    "The pruned model maintains an accuracy within 1% of the original model while reducing the model size by 40%."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
